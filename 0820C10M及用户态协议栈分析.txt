1：用户态协议栈
   dpdk/netmap
   c10M的问题，千万并发
2：nginx skynet  zeromq redis使用的是linux内核协议栈。  
	客户端发送数据，到应用程序的流程：
		a. kernel从网卡copy 数据到内核的协议栈 ==》dma拷贝
		b. 从内核协议栈copy 到应用程序。
3：优化：实现用户态协议栈，做到应用程序上。===》协议栈零拷贝
4：netmap 
	==》网卡的数据直接到达应用程序， 
			通过mmap, 
			或者PF_ring libcap (用的原生socket raw_socket)===》socket从网卡（链路层）中直接抓原生数据， 抓包时常用
				原生socket走协议栈，不走tcp，还是两次拷贝
				原生socket数据链路层的数据不一定是网卡的数据。
5：网卡也有内存， mmap把外设的数据直接映射到内存中 ===》dma的方式
			===》内存是进程调用mmap，进程里有这个数据，是虚拟内存  netmap是这种方式

	netmap是开源的框架，是在软件的层次上解决映射的问题。
		当网卡传输大量的数据时，netmap的映射有影响
	dpdk有商业团队的维护，且内部有内存管理（对接收的数据进行管理）等。 文档也比较全面。
6：C10M问题     C10K==》1万并发
	在没有epoll的情况下，实现并发 1：poll/select （多个select实现） 
								  2:多线程/多进程    
								  3：iocp，kqueue
	epoll（使用红黑树+链表）解决C10问题 ：
	C10K要解决的问题：
		1：数量   
		2：应用程序和内核的拷贝fd
		3：多线程内存占用比较多
	
	1：数量 ==》所有io集合的数量，把所有需要的网络io放在一起  红黑树（空间扩展问题，查找快，顺序） 
					==排除哈希，数组   排除b树（增删改查复杂，空间浪费，层高） b+树（浪费空间，叶子结点存储）
				不用排序，需要快速找到 ==》队列
			==》少量io就绪   
	2：拷贝 ==》一次一次添加到集合中，不是一次添加所有。
			三个接口  epoll_create()  epoll_ctl() 逐个io操作（select/poll没有）  epoll_wait()
			
		epoll是linux的一个补丁 patch，是linux早期设计的一个缺陷。

7：如何做到1000W的并发，以及更大？
	1千万的并发连接，100万个连接/秒（accept相应），10G/秒的连接，1千万个数据包/秒
	10微秒的延迟，10微秒的抖动，并发10核技术
	
	epoll能做到多少io的连接？ fd的数量？（红黑树能存fd数量，就绪队列无上限） ==》一般不是epoll的瓶颈

单机服务器：  ==》内存  CPU核数  SSD，磁盘响应    网络（网卡）  应用程序  操作系统
	1：通过多线程，多进程能解决吗？  不能~
	2：思考 ： 每个连接，如何接受，如何处理？   
					1个连接，4k的sendbuff，4k的recvbuff
					每个8k，则1K*1K*10*8 = 80G   ===》TB
				CPU： 1千万个数据包/秒  目前服务器每秒处理50K数据包 10KK--》200核
				网卡：千兆换万兆===》硬件有瓶颈
					  网卡的优化 ===》虚拟网卡
				操作系统： 减少拷贝，==》用户态协议栈，绕过内核，
8：用户态协议栈如何实现？ 如何设计一个协议栈？
	网卡 ==》 用户态协议栈==》应用程序
	
	ntytcp是通过netmap实现的
	
	网卡 NIC  netmap
	NIC是对网卡的封装，NIC是linux下的子系统，网卡的封装
	网卡eth0是对网卡的一个实例化
	netmap：是内核的驱动模块  ==》内核的支持模块
			是应用程序编程接口 ==》库
		==》在nic子系统的模块下，实现mmap的功能。
		
9：了解ntytcp的架构及实现
