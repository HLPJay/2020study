/******************************************
epoll是内核做的   用户处积累了大量的io
1:每一个IO对应哪些属性
2:很多个IO，我们应该如何去存储  ==>用一个块状的数组来存


把以前io为单位给成一个个的事件
把

在reactor模式下实现web服务器

reactor+线程池    最好的做法==》 recv后push进线程池 与业务分离
send recv 放在不通线程    在堆里加锁

看一下libevent的源码
用libevent找一个demo跑起来/看一下redis的源码

测试reactor的并发连接数量

TCP的发送缓冲区
*******************************************/
//epoll  reactor的写法
//每个io对应的属性  事件
#define BUFFER_LENGTH 1024
typedef int NCALLBACK(int , int , void*);
struct ntyevent{ //即每一个io对应一个这样的事件
	int fd;
	int events;//EPOLLIN EPOLLOUT  监控的事件
		//用来做与epoll做分离
	int *arg;
	int (*callback)(int fd, int events, void*arg);//回调函数  
	
	char buffer[BUFFER_LENGTH];  //这个buffer用于接收完整包
	int length;
	
	int status;
};


//一个反应堆包含    epfd   io事件的集合
struct ntyreactor{
	int epfd;
	struct ntyevent *events;//可以是堆，可以是红黑树     //send和recv放在不同的线程 的时候要加锁
};


//把网络io的放到   ---》转成 事件
//抽象成事件了
int nty_event_set(struct ntyevent *ev, int fd,NCALLBACK cb,  void *arg)
{
	ev->fd = fd;
	ev->callback = cb;
	ev->arg = arg;
	return 0;
}

//epoll operator
//accept新加一个clientfd   //把ev传进去
int nty_event_add(int epfd, int events， struct ntyevent *ev )
{
	struct epoll_event ep_ev ={0};
	ep_ev.data.ptr = ev;
	ep_ev.events = ev->events = events;
	
	int op = 0;
	if(ev->status == 1) //判断这个事件是否加入到epoll中
	{
		op = EPOLL_CTL_MOD;
	}else{
		op = EPOLL_CTL_ADD;
		ev->status = 1;
	}
	
	if(epoll_ctl(epfd, op, ev->fd, &ep_ev)<0)
	{
		return -1;
	}
}

//close ---》epoll_delete
//read /write --->epoll_update
//LT  小包一直触发的情况？（等等一次读完）  ===》删除
int nty_event_del(int epfd, int events， struct ntyevent *ev )
{
	struct epoll_event ep_ev = {0};
	if(ev->status != 1)
	{
		return -1;
	}
	
	ep_ev.data.pre = ev;
	ev->status = 0;
	
	epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);
}

//redis的源码
#define MAX_EPOLL_EVENTS 1024
int ntyreactor_run(struct ntyreactor *reactor)
{
	struct epoll_event events[MAX_EPOLL_EVENTS];
	while(1)
	{
		int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 5);
		if(nready<0)
		{
			continue;
		}
		int i=0; 
		for(i=0; i<nready; i++)
		{
			//用户空间event的集合 关注的可读还是可写事件是记住的
			struct ntyevent *ev = (struct ntyevent*)events[i].data.ptr;
			//read  如果没有关注EPOLLIN 不会触发  
			//我们关注的是可读   epoll返回的是可读  触发
			if((ev->events&EPOLLIN )&&(events[i].events& EPOLLIN))
			{
				ev->callback(ev->fd, events[i].events, ev->arg);
			}
			//write
			if((ev->events&EPOLLOUT )&&(events[i].events& EPOLLOUT))
			{
				ev->callback(ev->fd, events[i].events, ev->arg);
			}
		}
	}
}

//通过set进行关联
//send之前，关注io是否可写，若可写，直接触发   ===》一直触发，直到
int send_callback(int fd, int events, void*arg){
	struct ntyreactor* reactor = (struct ntyreactor*)arg;
	if(reactor ==NULL)
	{
		return -1;
	}
	struct ntyevent* ev = reactor->events +fd;
	int len = send(fd, ev->buffer, ev->length, 0);
	if(len>0)
	{
		//1:delete -->add
		//2:add  进行update
		printf("send %s \n", ev->buffer);
		nty_event_add(reactor->add, EPOLLIN, ev);
	}//返回-1   是buffer满了 不可写
	//一般不可能有发送一半的场景  没法送完，下一次发
}
//send是用户空间，拷贝到tcp内核buffer

//epollout --->send之前要判断
//分包和组包在recv上实现   多几次recv_callback
int recv_callback(int fd, int events, void*arg){
	struct ntyreactor* reactor = (struct ntyreactor*)arg;
	if(reactor ==NULL)
	{
		return -1;
	}
	struct ntyevent* ev = reactor->events+fd;
	//这个buffer用于接收完整包
	
	//没读完 ==>自行处理 和读完了
	int len = recv(fd, ev->buffer, BUFFER_LENGTH, 0);
	if(len>0)
	{
		ev->length = len;
		ev->buffer[len] = '\0'
		//1：没读完
		//2：读完了
		nty_event_set(ev, fd, send_callback, reactor);
		nty_event_add(reactor->epfd,EPOLLOUT, ev);
	}else{//
		close(ev->fd);
		nty_event_del(reactor->epfd, EPOLLIN, ev);
	}else{
		if(errno == EAGAIN || errno == EWOULDBLOCK) //被另外一个线程取了
		{
			continue;
		}
		close(ev->fd);
		nty_event_del(reactor->epfd, EPOLLIN, ev);
	}
}


//客户端调用connect的时候触发
int accept_callback(int fd, int events, void*arg){
	struct ntyreactor* reactor = (struct ntyreactor*)arg;
	if(reactor ==NULL)
	{
		return -1;
	}
	
	struct sockaddr_in client_addr;
	socklen_t len = sizeof(client_addr);
	int clientfd = accept(fd, (struct sockaddr *)&client_addr,&len);
	if(clientfd<0)
	{
		return -1;
	}
	
	fcntl(clientfd, F_SETEL, O_NONBLOCK);
	//accpet之后  设置一个事件  添加进去
	//accept之后 一般都是客户端先发给服务器端 服务器send也可以
	nty_event_set(reactor->events[clientfd], clientfd, recv_callback, reactor);
	nty_event_add(reactor->epfd, EPOLLIN. &reactor->events[clientfd]);
	return 0;
}

//reactor的初始化

int ntyreactor_init(struct ntyreactor *reactor)
{
	if(reactor == NULL)
	{
		return -1;
	}
	memset(reactor, 0, sizeof(struct ntyreactor));
	
	reactor->epfd = epoll_create(1);
	if(reactor->epfd <0)
	{
		return -2;
	}
	reactor ->events = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS)*sizeof(struct ntyevent ));
	if(reactor->events == NULL)
	{
		close(reactor->epfd);
		return -3;
	}
	return 0;
}
//reactor 管理众多事件的
int ntyreactor_destroy(struct ntyreactor *reactor)
{
	close(reactor->epfd);
	free(reactor->events);
}

int init_sock(short port)
{
	int sockfd = socket(AF_INET, SOCK_STREAM, 0);
	if(sockfd<0) return -1;
	
	fcntl(fd, F_SETEL, O_NONBLOCK);

	struct sockaddr_in serv_addr;
	memset(&serv_addr, 0, sizeof(serv_addr));
	serv_addr.sin_family = AF_INET;
	serv_addr.sin_port = htons(port);
	serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);
	
	if(bind(sockfd, (struct sockaddr*)&serv_addr,sizeof(struct sockaddr))<0)
	{
		perror("bind");
		return -2;
	}

	if(listen(sockfd, 10) <0)
	{
		perror("listen");
		return -3;
	}
	return sockfd;
}
//设置一个事件 listen
int ntyreactor_addlistener(struct ntyreactor * ractor, int sockfd, NCALLBACK *accpet)
{
	nty_event_set(ractor->events[sockfd], sockfd, accpet, ractor );
	nty_event_add(ractor->epfd, EPOLLIN, ractor);
}

int main()
{
	unsigned short port = 8888;
	int sockfd = init_sock(port);
	
	struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor));
	
	ntyreactor_init();
	ntyreactor_addlistener(reactor, sockfd, accept_callback);
	ntyreactor_run(reactor);//loop
	
	ntyreactor_destroy(reactor);
	close(sockfd);
	return 0;
	
}
//存io的集合  epoll 
