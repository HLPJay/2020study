0707日学习：
	1：使用C++实现简单的写入excel ==》csv
	2：om项目布置
	3：log4Cplus测试
	4：并发实现
	
敌方阵营已攻克，开始揭竿收枪炮。
CMAKE：
	ZLToolKit.git ==》C++11实现的
makefile：
	NtyCo.git
根据项目学习makefile和cmake：	
	https://github.com/xiongziliang/ZLToolKit.git
	https://github.com/wangbojing/NtyCo.git

makefile不止编译，可以输出
	1：targets 默认第一个，不是all
		要注意tab的使用，空格和不加tab都会有问题
	2：
  
  
  
  一个很多个很多个脚丫子的，一个圈上有一个眼睛，一直围着一个圈圈在努力奔跑。
圈圈上写着各种理想和要奋斗的事
然后镜头转变，慢慢延伸，延伸出窗外，慢慢扩大到整个城市。


睡/不醒
知道抽烟不好，所以不去抽烟
知道世俗险恶，所以不愿入世
孤芳自赏，孤冷自傲

https://mp.weixin.qq.com/s?__biz=MzA3ODgyNzcwMw==&mid=202113096&idx=1&sn=7ce616f596c529890dfd475ce8d31858&scene=4##
C10M的思想就是将控制层留给Linux做，其它数据层全部由应用程序来处理。没有线程调度、没有系统调用、没有中断等，当你的程序仍运行在Linux用户空间，并仅仅对数据进行高效的分析和处理。

网卡：摒弃Linux内核协议栈，可以使用PF_RING，Netmap，intelDPDK来自己实现驱动；
CPU：使用多核编程技术替代多线程，将OS绑在指定核上运行；
内存：使用大页面，减少访问；

DPDK应用程序是运行在用户空间上利用自身提供的数据平面库来收发数据包，绕过了Linux内核协议栈对数据包处理过程。Linux内核将DPDK应用程序看作是一个普通的用户态进程，包括它的编译、连接和加载方式和普通程序没有什么两样。DPDK程序启动后只能有一个主线程，然后创建一些子线程并绑定到指定CPU核心上运行。

DPDK使用无中断方式直接操作网卡的接收和发送队列（除了链路状态通知仍必须采用中断方式以外）
PMD驱动从网卡上接收到数据包后，会直接通过DMA方式传输到预分配的内存中，同时更新无锁环形队列中的数据包指针，不断轮询的应用程序很快就能感知收到数据包，并在预分配的内存地址上直接处理数据包，这个过程非常简洁
DPDK目前支持了2M和1G两种方式的hugepage。

多线程的初衷是提高整体应用程序的性能，但是如果不加注意，就会将多线程的创建和销毁开销，锁竞争，访存冲突，cache失效，上下文切换等诸多消耗性能的因素引入进来。这也是Ngnix使用单线程模型能获得比Apache多线程下性能更高的秘籍。
多核编程和多线程有很大的不同：多线程是指每个CPU上可以运行多个线程，涉及到线程调度、锁机制以及上下文的切换；而多核则是每个CPU核一个线程，核心之间访问数据无需上锁。


为了得到千万级并发，DPDK使用如下技术来达到目的：使用PMD替代中断模式；将每一个进程单独绑定到一个核心上，并让CPU从这些核上隔离开来；批量操作来减少内存和PCI设备的访问；使用预取和对齐方式来提供CPU执行效率；减少多核之间的数据共享并使用无锁队列；使用大页面。

除了UDP服务器程序，DPDK还有很多的场景能应用得上。一些需要处理海量数据包的应用场景都可以用上，包括但不局限于以下场景：NAT设备，负载均衡设备，IPS/IDS检测系统，TOR（Top of Rack）交换机，防火墙等，甚至web cache和web server也可以基于DPDK来极大地提高性能。

其它性能优化技术
4.1减少内存访问
运算指令的执行速度是非常快，大多数在一个CPU cycle内就能完成，甚至通过流水线一个cycle能完成多条指令。但在实际执行过程中，处理器需要花费大量的时间去存储器来取指令和数据，在获取到数据之前，处理器基本处于空闲状态。那么为了提高性能，缩短服务器响应时间，我们可以怎样来减少访存操作呢？


（1）少用数组和指针，多用局部变量。因为简单的局部变量会放到寄存器中，而数组和指针都必须通过内存访问才能获取数据；

（2）少用全局变量。全局变量被多个模块或函数使用，不会放到寄存器中。

（3）一次多访问一些数据。就好比我们出去买东西一样，一次多带一些东西更省时间。我们可以使用多操作数的指令，来提高计算效率，DPDK最新版本配合向量指令集（AVX）可以使CPU处理数据包性能提升10%以上。

（4）自己管理内存分配。频繁调用malloc和free函数是导致性能降低的重要原因，不仅仅是函数调用本身非常耗时，而且会导致大量内存碎片。由于空间比较分散，也进一步增大了cache misses的概率。

（5）进程间传递指针而非整个数据块。在高速处理数据包过程中特别需要注意，前端线程和后端线程尽量在同一个内存地址来操作数据包，而不应该进行多余拷贝，这也是Linux系统无法处理百万级并发响应的根本原因，有兴趣的可以搜索“零拷贝”的相关文章。


4.2Cache大小的影响
熟知cache的大小，了解程序运行的时间和空间上局部性原来，对于我们合理利用cache，提升性能非常重要。同时要少用静态变量，因为静态变量分配在全局数据段，在一个反复调用的函数内访问该变量会导致cache的频繁换入换出，而如果是使用堆栈上的局部变量，函数每次调用时CPU可以直接在缓存中命中它。最后，循环体要简单，指令cache也仅仅有几K，过长的循环体会导致多次从内存中读取指令，cache优势荡然无存。
4.3避免False Sharing
多核CPU中每个核都拥有自己的L1/L2 cache，当运行多线程程序时，尽管算法上不需要共享变量，但实际执行中两个线程访问同一cache line的数据时就会引起冲突，每个线程在读取自己的数据时也会把别人的cacheline读进来，这时一个核修改改变量，CPU的cache一致性算法会迫使另一个核的cache中包含该变量所在的cache line无效，这就产生了false sharing（伪共享）问题。

4.4对齐

首先谈到的是内存对齐：根据不同存储硬件的配置来优化程序，性能也能够得到极大的提升。在硬件层次，确保对象位于不同channel和rank的起始地址，这样能保证对象并并行加载。
4.5了解NUMA

NUMA系统节点一般是由一组CPU和本地内存组成。NUMA调度器负责将进程在同一节点的CPU间调度，除非负载太高，才迁移到其它节点，但这会导致数据访问延时增大。下图是2颗CPU支持NUMA架构的示意图，每颗CPU物理上有4个核心。
4.6减少进程上下文切换
4.7分支预测
4.8利用流水线并发
4.9适度预取
4.10合理编写循环

尽量减少函数调用，每一次调用都要进行压栈、保存寄存器和执行指令跳转等都会耗费不少时间，可以将一些小的函数写成内联，或直接用宏或语句代替。


对于提高性能空间换时间也是一个非常重要的设计理念，Bloom Filter就是一个典型的例子，还有一些位图、hash的思想也是不错的选择。


在高性能程序设计时，要减少过保护，除了会影响程序执行的一些关键路径和参数要进行校验外，其它参数不一定非得要检查，毕竟错误情况是少数。


尽量用整型代替浮点数，少用乘除、求余运算，这些操作会占用更多的CPU周期，能提前计算的表达式要提前计算出结果。


充分利用编译器选项，-O3帮助你进行文件内部最深层次的优化，使用其它编译器如icc能编译出在x86平台下运行更快的程序。


延时计算，最近用不上的变量就不要去初始化，操作系统为了提高性能也使用COW(copy-on-write)策略，在fork子进程的时候不会立即复制进程的所有页表。

