事件通知库libevent服务器百万并发测试：
可以装一个虚拟机 拷贝三分测试
	1：服务器并发量：同一时间服务器能够同时承载的客户端数量。  ==》这个是对的
					 服务器处理客户端请求的数量     ===》
					 单位时间内，能够处理的请求数量  ===》服务器每秒的吞吐量 qps
		并发量：服务器能承载这些fd。==》维护fd
	2：百万并发：
		服务器能够承载fd ==》维护fd   ===》最基本的条件
		每秒5w以上的响应请求
		对数据库的操作
		磁盘的操作（日志）
		CPU占用率 ==》不能超过60%
		内存占用率 ===》不能超过80%
	3：实现最基本的，支持百万个fd：
		reactor模式下，能够同时承载100w的fd？===》redis和libeveng库
		mul_port_client_epoll.c做客户端
		reactor.c做服务端
		
		./reactor 6023
		./mul_port_client_epoll 服务器ip 端口
		
		发现服务器跑不上去。 1024 ==》什么都不改的情况
		
		1：操作系统对fd的限制
			ulimit -a 查看open_files限制   ===》每个进程能够最多打开的fd的数量
				修改这个配置文件可以修改vi /etc/security/limits.conf
			file-max: fd的最大值限制
				sysctl -p
				vi /ect/sysctl.conf
				sysctl -p  ==>命令使修改生效 把值刷新到/proc/net  ===》？？？
		2：原代码有问题，
				MAX_EPOLL_EVENTS ==》每一个事件的数量变大，把这个宏改大
				
				reactor 核心  把io转换为管理事件events，
				cat /proc/sys/net/ipv4/tcp_wmem ==》查看
				
				把宏改为1024*1024
				大内存申请用posix_memalign
			两个Client不断起连接
		3：端口的问题： ===》同时listen多个端口？  为什么？
				测试的时候服务器和客户端其实没有断开，长连接
				每一个fd，对应一个控制块，五元组（sip,dsp,sport,dport,proto）
													2  1    6w  5(监听)    1(tcp)
					测试的时候要用多个端口，实际环境中不用。
						fd的唯一由五元组保证，外网中每个ip都不一样，保证唯一。
													
				客户端已经关闭。===》？？？
				fd的唯一就是五元组中这些值相乘的组合数。
			假设是端口号的问题，则加一个listen。
				源码去掉超时，增加几个listen端口。 ===》多listen几个端口  初始化加listen，要注意关闭
				服务器端也增加多几个端口
			listen多个端口   ==》nginx里 nginx.conf-->server 也是
				可以跑到25w左右
		4：服务器和客户端都不动，死了：  64999 客户端==>??65535
			猜测：	
				1：随机端口（1024~65535的端口）满了
				2：计算机文件fd 到了上限？？   ==》计算机文件数    + 64999 = 65535？？？
				
				cat /proc/sys/fs/file-max
				
				客户端，每台机器本地端口是有限的，端口是16位，所以0~65535 0~1023是预留端口，所以可用1024~65535，也就是64511个
				
				cat /proc/sys/net/ipv4/ip_local_port_range  ==>查看端口范围
			1：不是服务器问题
			2：	sudo sysctl -p
				    net.nf_conntrack_max  ===>每一个连接对应的最大的track，解决了64999的限制
				sudo modprobe ip_conntrack ==>需要加载这个模块才能sudo sysctl -p查出来
					==》处理了这个问题
					track   操作系统默认不给的 ==》客户端和服务器三次握手，最后一次链接限制的数量
				
			===》观察流程里的耗时，如何将每1000个链接耗时最短？ 
				===》1：由accept限制
					用多个线程/进程，多个accept可以解决。==》越往下，性能越优
					1：多个accept在一个线程里。      
					2：多个accept分配在不同的线程里。 --》每个线程都有一个listen
					3：多个accept分配在不同的进程里。 --》nginx做法，每个进程独立资源  ==》适合前后请求没有关系的，两次io请求
				多线程与多进程的区别：
					1：多进程，不需要加锁。    ==》共用资源或者a用b的方式   用共享内存的方式比较简单
					2：每个进程fd的上限比线程要多。   fd的数量可以做到更多。
					
				多线程是以时间片进行？？ 什么意思？
		关于网络io操作：  ===>epoll ,多线程，多进程等  如何做出好的网络IO的模型 
								==》参考课件，十种实现方案
			1：accept()
			2：send()
			3：recv()
			4：close()
			
		5：分析  五元组拿出来分析
		客户端出现问题：140999
			connect:Cannot assign requested addrss
			error: Cannot assign requested addrss
		问题原因，服务器端口满了，原先设置是监听5个端口，改成50个
					客户端也要改连接多个    改成50个    ===》event只开到50万    1024*64
		
			1024*1024会出问题？？？  events？
		测试的时候必须开多个端口，外网不用，client的ip都不一样，不需要~？？？
		
		
		外网要经过路由器  测试时耗时比较长
		
		影响服务器性能的因素，
			C10K --》C1000K ---》C10M
			1：五元组分析
				（CPU，内存，网络带宽，操作系统，应用程序）（磁盘）
			可以通过命令查看，也可以直接扩展cpu，内存。
			C10当时的问题是由于操作系统引起 --》epoll解决  ==》epoll_wait只拷贝活跃的？  ==》徒手造轮子，实现epoll
			现在C1000K  ==》百万级别并发 ==》epoll解决
			C10M      ===》用户态IO
					==》通过网卡，拷贝到协议栈（内核），再到用户应用程序==》每个包都经过两次copy
						==》减少协议栈的copy，直接网卡到内存  （用户态协议栈）
							==》零拷贝，DPDK，网卡映射？
							mmap映射到
						ntytcp测试跑到一百万以上
			
			并发，连接，百分之十以上活跃并且可以执行程序。
			epoll没有用到共享内存
			
		6：客户端276999挂死，===》内存原因，通过posix_alignmem分配大块内存
		
		
		协议栈：多队列网卡和用户态协议栈
		select/poll/epoll ===>reactor  ===》测试并发量？
								在reactor上实现http
								reactor如实实现业务
